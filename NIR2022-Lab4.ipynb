{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIR 2022 - Lab 4: LMs & Query Rewriting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we will see how to run approaches based on language models and query rewriting in PyTerrier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systems Setup\n",
    "\n",
    "We will start by building an index of our data collection and a few systems in PyTerrier.\n",
    "This step is only required to obtain system outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2453, 2)\n",
      "     docno                                               text\n",
      "0   935016  he emigrated to france with his family in 1956...\n",
      "1  2360440  after being ambushed by the germans in novembe...\n",
      "2   347765  she was the second ship named for captain alex...\n",
      "3  1969335  world war ii was a global war that was under w...\n",
      "4  1576938  the ship was ordered on 2 april 1942 laid down...\n",
      "(9, 2)\n",
      "       qid                 query\n",
      "0  1015979    president of chile\n",
      "1     2674    computer animation\n",
      "2   340095  2020 summer olympics\n",
      "3  1502917         train station\n",
      "4     2574       chinese cuisine\n",
      "(2454, 4)\n",
      "       qid    docno label iteration\n",
      "0  1015979  1015979     2         0\n",
      "1  1015979  2226456     1         0\n",
      "2  1015979  1514612     1         0\n",
      "3  1015979  1119171     1         0\n",
      "4  1015979  1053174     1         0\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "import pandas as pd\n",
    "\n",
    "# corpus\n",
    "docs_df = pd.read_csv('data/lab_docs.csv', dtype=str)\n",
    "print(docs_df.shape)\n",
    "print(docs_df.head())\n",
    "\n",
    "# topics\n",
    "topics_df = pd.read_csv('data/lab_topics.csv', dtype=str)\n",
    "print(topics_df.shape)\n",
    "print(topics_df.head())\n",
    "\n",
    "# Load qrels\n",
    "qrels_df = pd.read_csv('data/lab_qrels.csv', dtype=str)\n",
    "print(qrels_df.shape)\n",
    "print(qrels_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init PyTerrier\n",
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "    pt.init(boot_packages=[\"com.github.terrierteam:terrier-prf:-SNAPSHOT\"])  # Initialisation package for RM3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2453\n",
      "Number of terms: 23693\n",
      "Number of postings: 208487\n",
      "Number of fields: 0\n",
      "Number of tokens: 273373\n",
      "Field names: []\n",
      "Positions:   true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build index\n",
    "indexer = pt.DFIndexer(\"./indexes/default\", overwrite=True, blocks=True)\n",
    "index_ref = indexer.index(docs_df[\"text\"], docs_df[\"docno\"])\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "print(index.getCollectionStatistics().toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2453\n",
      "Number of terms: 23693\n",
      "Number of postings: 208487\n",
      "Number of fields: 0\n",
      "Number of tokens: 273373\n",
      "Field names: []\n",
      "Positions:   true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load index\n",
    "index_ref = pt.IndexRef.of(\"./indexes/default/data.properties\")\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "print(index.getCollectionStatistics().toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build IR systems\n",
    "tf = pt.BatchRetrieve(index, wmodel=\"Tf\")\n",
    "tfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>map</th>\n",
       "      <th>ndcg</th>\n",
       "      <th>ndcg_cut_10</th>\n",
       "      <th>P_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TF</td>\n",
       "      <td>0.610184</td>\n",
       "      <td>0.789583</td>\n",
       "      <td>0.851008</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.622287</td>\n",
       "      <td>0.798228</td>\n",
       "      <td>0.840808</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.628454</td>\n",
       "      <td>0.800955</td>\n",
       "      <td>0.842503</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name       map      ndcg  ndcg_cut_10      P_10\n",
       "0      TF  0.610184  0.789583     0.851008  0.800000\n",
       "1  TF-IDF  0.622287  0.798228     0.840808  0.766667\n",
       "2    BM25  0.628454  0.800955     0.842503  0.766667"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate systems on the first three topics using the PyTerrier Experiment interface\n",
    "qrels_df = qrels_df.astype({'label': 'int32'})\n",
    "pt.Experiment(\n",
    "    retr_systems=[tf, tfidf, bm25,],\n",
    "    names=['TF', 'TF-IDF', 'BM25'],\n",
    "    topics=topics_df,\n",
    "    qrels=qrels_df,\n",
    "    eval_metrics=[\"map\", \"ndcg\", \"ndcg_cut_10\", \"P_10\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models\n",
    "\n",
    "Language models (LMs) are an alternative view to the ranking problem.\n",
    "In LMs, the actual occurrences of terms in a document are compared with the expected occurrences predicted from the characteristics of the collection and the document.\n",
    "\n",
    "Given a document $d$, that is assumed to be relevant, we develop a LM to estimate $p(q|d)$, the probability that query $q$ would be entered to retrieve document $d$. Documents are then ranked according to these probabilities.\n",
    "\n",
    "In practice, given a document $d$, the goal is to construct a document LM for the queries that may be entered to retrieve it.\n",
    "The simplest document LM is based on the counts of the terms appearing in the document (maskimum likelihood model).\n",
    "However, given the limited number of terms within a document, the resulting estimates (i) might be wildly inaccurate, and (ii) equal to $0$ for all the terms not appearing in the document.\n",
    "To address these problems, a common approach in information retrieval is to use _smoothing_ techniques.\n",
    "\n",
    "In this notebook, we will use PyTerrier's implementation of the [Dirichlet LM](http://terrier.org/docs/v4.0/javadoc/org/terrier/matching/models/DirichletLM.html), a language model with Dirichlet smoothing, a technique that pretends that each document has an extra $\\mu>0$ tokens in each document.\n",
    "As a result, the impact of additional terms depends on the length of a given document: the longer the document, the lower the impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dirichlet Language Model\n",
    "dlm = pt.BatchRetrieve(index, wmodel=\"DirichletLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTerrier, the deafult value of $\\mu$ is $\\mu=2500$.\n",
    "During the class, however, we have seen that a common approach is to set $\\mu$ to the average document length in the collection.\n",
    "Compute the average document length $ADL$ and evaluate a Dirichlet LM with $\\mu=ADL$.\n",
    "\n",
    "Tips: \n",
    "- Remember that you can access a `document index` within a PyTerrier `index`. \n",
    "- Also, PyTerrier assign an incremental document id to each document in the collection, starting from $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dirichlet Language Model with parameter mu equal to average document length\n",
    "\n",
    "avg = 10\n",
    "\n",
    "dlm_avg = pt.BatchRetrieve(index, wmodel=\"DirichletLM\", controls={\"c\": avg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>map</th>\n",
       "      <th>ndcg</th>\n",
       "      <th>ndcg_cut_10</th>\n",
       "      <th>P_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TF</td>\n",
       "      <td>0.610184</td>\n",
       "      <td>0.789583</td>\n",
       "      <td>0.851008</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.622287</td>\n",
       "      <td>0.798228</td>\n",
       "      <td>0.840808</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.628454</td>\n",
       "      <td>0.800955</td>\n",
       "      <td>0.842503</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DLM_default</td>\n",
       "      <td>0.644624</td>\n",
       "      <td>0.812032</td>\n",
       "      <td>0.865656</td>\n",
       "      <td>0.788889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DLM_avg</td>\n",
       "      <td>0.644624</td>\n",
       "      <td>0.812032</td>\n",
       "      <td>0.865656</td>\n",
       "      <td>0.788889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name       map      ndcg  ndcg_cut_10      P_10\n",
       "0           TF  0.610184  0.789583     0.851008  0.800000\n",
       "1       TF-IDF  0.622287  0.798228     0.840808  0.766667\n",
       "2         BM25  0.628454  0.800955     0.842503  0.766667\n",
       "3  DLM_default  0.644624  0.812032     0.865656  0.788889\n",
       "4      DLM_avg  0.644624  0.812032     0.865656  0.788889"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate systems on the topics using the PyTerrier Experiment interface\n",
    "pt.Experiment(\n",
    "    retr_systems=[tf, tfidf, bm25, dlm, dlm_avg],\n",
    "    names=['TF', 'TF-IDF', 'BM25', 'DLM_default', 'DLM_avg'],\n",
    "    topics=topics_df,\n",
    "    qrels=qrels_df,\n",
    "    eval_metrics=[\"map\", \"ndcg\", \"ndcg_cut_10\", \"P_10\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "Before looking at query rewriting techniques, we will have a look at PyTerrier pipelines, which will be used in the rest of this notebook.\n",
    "\n",
    "Part of the power of PyTerrier comes indeed from having these pipelines, that allow to easily formulate complex retrieval pipelines.\n",
    "Particularly relevant to us, today, is the chaining pipeline, which is obtained with the `>>` (Then) operator.\n",
    "This operator allows us to perform multiple steps one after the other.\n",
    "\n",
    "For example, as we will see later in the course, a typical approach consists of re-ranking documents that were first retrieved by another system.\n",
    "Given a query $Q$ and two ranking systems $\\text{R}_1$ and $\\text{R}_2$, we can perform the following pipeline:\n",
    "$$ R' = \\text{R}_2(\\text{R}_1(Q)))$$\n",
    "through the `>>` operator:\n",
    "$$\\text{Pipeline} = R_1 >> R_2$$\n",
    "$$R' = \\text{Pipeline}(Q)$$\n",
    "where $R'$ is the list of ranked documents.\n",
    "\n",
    "In the following, we will be adding query expansion as an intermediate step between re-ranking in order to perform ranking with pseudo-relevance feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Rewriting\n",
    "\n",
    "Query rewriting consists of reformulating the original query in order to improve the effectiveness of a ranking system. \n",
    "The basic idea behind this technique is to return relevant documents even if there is no term that matches with the original query, hence possibly increasing the quality of the ranking.\n",
    "\n",
    "Although queries can be rewritten by users, in this notebook, we will look at automatic techniques to query rewriting.\n",
    "In particular, PyTerrier differentiates between two forms of query rewriting:\n",
    "\n",
    "- Query expansion `Q -> Q`: this rewrites a query $Q$, for instance by adding/removing extra query terms. \n",
    "\n",
    "- Pseudo-relevance feedback `R -> Q`: this rewrites a query $Q$ by making use of an associated set of documents $R$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTerrier offers native access to Metzler and Croft’s sequential dependence model (`sdm`), designed to boost the scores of documents where the query terms occur in close proximity.\n",
    "This technique rewrites each input query such that:\n",
    "- pairs of adjacent query terms are added as #1 and #uw8 complex query terms, with a low weight\n",
    "- the full query is added as #uw12 complex query term, with a low weight.\n",
    "- all terms are weighted by a proximity model, such as the Dirichlet LM\n",
    "\n",
    "For example, the query \"pyterrier IR platform\" would become \"pyterrier IR platform #1(pyterrier IR) #1(IR platform) #uw8(pyterrier IR) #uw8(IR platform) #uw12(pyterrier IR platform)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, expand queries through the sequential dependence model\n",
    "# Then, use BM25 to rank the documents in the collection using the expanded queries\n",
    "sdm = pt.rewrite.SequentialDependence()\n",
    "sdm_bm25 = sdm >> bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at how the query \"\"black wall\" is expanded by SDM as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdm_bm25.search(\"black wall\").iloc[0]['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate systems on the topics using the PyTerrier Experiment interface\n",
    "pt.Experiment(\n",
    "    retr_systems=[tf, tfidf, bm25, dlm, dlm_avg, sdm_bm25],\n",
    "    names=['TF', 'TF-IDF', 'BM25', 'DLM_default', 'DLM_avg', 'SDM >> BM25'],\n",
    "    topics=topics_df,\n",
    "    qrels=qrels_df,\n",
    "    eval_metrics=[\"map\", \"ndcg\", \"ndcg_cut_10\", \"P_10\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Expansion with word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular approach to query expansion is to use word vectors, such as Word2Vec or Glove, to retrieve similar words to the ones in the original query.\n",
    "While we will cover word vectors in the upcoming classes, we see here how we can use them to perform query expansion.\n",
    "\n",
    "In particular, we use the Gensim library to download 50-dimensional Glove word vectors and retrieve $k$ similar words for each term in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load Glove vectors\n",
    "model = api.load('glove-wiki-gigaword-50')\n",
    "print(model.most_similar('tree'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand each query by adding the top-k similar words for each word in the query\n",
    "k = 10\n",
    "\n",
    "topics_qe_df = topics_df.copy()\n",
    "for i in range(len(topics_qe_df)):\n",
    "    q = topics_qe_df.iloc[i]['query']\n",
    "    qe = []\n",
    "    for word in q.split(' '):\n",
    "        expanded_words = [pair[0] for pair in model.most_similar(word, topn=k)]\n",
    "        expanded_words.append(word)\n",
    "        qe.append(expanded_words)\n",
    "    topics_qe_df.iloc[i]['query'] = gensim.parsing.preprocessing.remove_stopwords(\" \".join([e for l in qe for e in l]))\n",
    "\n",
    "topics_qe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original query\n",
    "topics_df.iloc[1]['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanded query\n",
    "topics_qe_df.iloc[1]['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate previous systems on the expanded topics\n",
    "pt.Experiment(\n",
    "    retr_systems=[tf, tfidf, bm25, dlm, dlm_avg],\n",
    "    names=['TF', 'TF-IDF', 'BM25', 'DLM_default', 'DLM_avg'],\n",
    "    topics=topics_qe_df,\n",
    "    qrels=qrels_df,\n",
    "    eval_metrics=[\"map\", \"ndcg\", \"ndcg_cut_10\", \"P_10\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo-relevance Feedback\n",
    "\n",
    "Finally, we now look at pseudo-relevance feedback approaches.\n",
    "\n",
    "Pseudo-relevance feedback allows to re-rank documents without requiring the user to select relevant documents among the ones retrieved.\n",
    "It does so by assume that the top-$k$ ranked documents are relevant, and then applies relevance feedback under this assumption.\n",
    "\n",
    "PyTerrier offers native access to several approaches to pseudo-relevance feedback.\n",
    "Here, we will look at RM3, one of the most popular and effective approaches.\n",
    "\n",
    "The following parameters can be used to tune the behavioor of the model:\n",
    "- `fb_terms`: number of feedback terms (Defaults to 10)\n",
    "- `fb_docs`: number of feedback documents (Defaults to 3)\n",
    "- `fb_lambda`: lambda in RM3, i.e. importance of relevance model viz feedback model (Defaults to 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank by BM25, then apply query rewriting with RM3, finally re-rank the documents with BM25 using rewritten queries\n",
    "rm3 = pt.rewrite.RM3(index)\n",
    "rm3_pipe = bm25 >> rm3 >> bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate systems\n",
    "pt.Experiment(\n",
    "    retr_systems=[tf, tfidf, bm25, dlm, dlm_avg, sdm_bm25, rm3_pipe],\n",
    "    names=['TF', 'TF-IDF', 'BM25', 'DLM_default', 'DLM_avg', 'SDM >> BM25', 'BM25 >> RM3 >> BM25'],\n",
    "    topics=topics_df,\n",
    "    qrels=qrels_df,\n",
    "    eval_metrics=[\"map\", \"ndcg\", \"ndcg_cut_10\", \"P_10\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion Techniques\n",
    "\n",
    "As a last, quick task in this lab, we will look at a rank fusion method.\n",
    "\n",
    "Rank fusion refers to the technique that merges multiple system runs to produce a single top-$k$ list.\n",
    "Widely used approaches are:\n",
    "- CombSUM (score-based)\n",
    "- Borda Count (rank-based)\n",
    "- Reciprocal Rank Fusion (rank-based)\n",
    "\n",
    "We now look at CombSUM, which combines multiple system runs with a (weighted) sum of their scores.\n",
    "This can be trivially implemented in PyTerrier with the `+` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_plus_tfidf = 1.0*bm25 + 1.0*tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate systems\n",
    "pt.Experiment(\n",
    "    retr_systems=[tf, tfidf, bm25, dlm, dlm_avg, sdm_bm25, rm3_pipe, bm25_plus_tfidf],\n",
    "    names=['TF', 'TF-IDF', 'BM25', 'DLM_default', 'DLM_avg', 'SDM >> BM25', 'BM25 >> RM3 >> BM25', 'CombSUM'],\n",
    "    topics=topics_df,\n",
    "    qrels=qrels_df,\n",
    "    eval_metrics=[\"map\", \"ndcg\", \"ndcg_cut_10\", \"P_10\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}