{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIR 2022 - Lab 3: Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lab 2, we have seen how to index a collection of documents and how to search the index with different systems in PyTerrier.\n",
    "At the end of Lab 2, we also saw how to evaluate the performance of the different systems using standard metrics such as MAP and NDCG.\n",
    "\n",
    "Today, we will take a closer look at standard evaluation metrics.\n",
    "In particular, we will see how to use `pytrec_eval`, a Python library to evaluate on TREC-like data whether you use PyTerrier or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systems Setup\n",
    "\n",
    "We will start by building an index of our data collection and a few systems in PyTerrier.\n",
    "This step is only required to obtain system outputs.\n",
    "\n",
    "As we will see shortly, `pytrec_eval` only needs access to output files, which can be obtained in any other way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import pandas as pd\n",
    "\n",
    "# corpus\n",
    "docs_df = pd.read_csv('data/lab_docs.csv', dtype=str)\n",
    "print(docs_df.shape)\n",
    "print(docs_df.head())\n",
    "\n",
    "# topics\n",
    "topics_df = pd.read_csv('data/lab_topics.csv', dtype=str)\n",
    "print(topics_df.shape)\n",
    "print(topics_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init PyTerrier\n",
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "    pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index\n",
    "indexer = pt.DFIndexer(\"./indexes/default\", overwrite=True, blocks=True)\n",
    "index_ref = indexer.index(docs_df[\"text\"], docs_df[\"docno\"])\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "print(index.getCollectionStatistics().toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build IR systems\n",
    "tf = pt.BatchRetrieve(index, wmodel=\"Tf\")\n",
    "tfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search and Evaluate in PyTerrier\n",
    "\n",
    "In PyTerrier, we can use `search()` to search for documents relevant for a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the index for a query using TF-IDF model\n",
    "tfidf.search(\"black wall\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also search for multiple queries at once by grouping them in a Pandas DataFrame and then using the `transform()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the index for multiple queries using TF-IDF model\n",
    "queries = pd.DataFrame([[\"q1\", \"dragon\"], [\"q2\", \"wall\"]], columns=[\"qid\", \"query\"])\n",
    "tfidf.transform(queries).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, PyTerrier provides an interface for evaluating the performance of IR systems through the `Experiment` abstraction.\n",
    "Behind the scenes, `pt.Experiment` uses the `pytrec_eval` library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels_df = pd.read_csv('data/lab_qrels.csv', dtype=str)\n",
    "qrels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate systems on the first three topics using the PyTerrier Experiment interface\n",
    "qrels_df = qrels_df.astype({'label': 'int32'})\n",
    "pt.Experiment(\n",
    "    retr_systems=[tf, tfidf, bm25],\n",
    "    names=['TF', 'TF-IDF', 'BM25'],\n",
    "    topics=topics_df[:3],\n",
    "    qrels=qrels_df,\n",
    "    eval_metrics=[\"map\", \"ndcg\", \"ndcg_cut_10\", \"P_10\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers & Operators\n",
    "\n",
    "You'll have noted that BatchRetrieve has a `transform()` method that takes as input a dataframe, and returns another dataframe, which is somehow a *transformation* of the earlier dataframe (e.g., a retrieval transformation). In fact, `BatchRetrieve` is just one of many similar objects in PyTerrier, which we call [transformers](https://pyterrier.readthedocs.io/en/latest/transformer.html) (represented by the `TransformerBase` class).\n",
    "\n",
    "Let's give a look at a `BatchRetrieve` transformer, starting with one for the TF_IDF weighting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check tfidf is a transformer...\n",
    "print(isinstance(tfidf, pt.transformer.TransformerBase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this prints the type hierarchy of the TF_IDF class\n",
    "tfidf.__class__.__mro__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting capability of all transformers is that they can be combined using Python operators (this is called operator overloading).\n",
    "\n",
    "Concretely, imagine that you want to chain transformers together – e.g. rank documents first by Tf then re-ranked the exact same documents by TF_IDF. We can do this using the >> operator – we call this composition, or \"then\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's define a pipeline \n",
    "pipeline = tf >> tfidf\n",
    "print(isinstance(tfidf, pt.transformer.TransformerBase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.search(\"black wall\"))\n",
    "print(pipeline.search(\"black wall\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Task – Pipeline Construction\n",
    "\n",
    "Create a ranker that performs the follinwg:\n",
    " - obtains the top 10 highest scoring documents by term frequency (`wmodel=\"Tf\"`)\n",
    " - obtains the top 10 highest scoring documents by TF.IDF (`wmodel=\"TF_IDF\"`)\n",
    " - reranks only those documents found in BOTH of the previous retrieval settings using BM25.\n",
    "\n",
    "How many documents are retrieved by this full pipeline for the query `\"black wall\"`. \n",
    "\n",
    "If you obtain the correct solution, the document with docid `'1357'` should have a score 14.5976"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving system ouputs\n",
    "\n",
    "We now save the output of each query onto disk so we can later evaluate it with `pytrec_eval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save system rankings in TREC format\n",
    "# qid Q0 docno rank score tag\n",
    "tf_run = []\n",
    "for _, row in topics_df.iterrows():\n",
    "    qid, query = row\n",
    "    res_df = tf.search(query)\n",
    "    for _, res_row in res_df.iterrows():\n",
    "        _, docid, docno, rank, score, query = res_row\n",
    "        row_str = f\"{qid} 0 {docno} {rank} {score} tfidf\"\n",
    "        tf_run.append(row_str)\n",
    "with open(\"outputs/tf.run\", \"w\") as f:\n",
    "    for l in tf_run:\n",
    "        f.write(l + \"\\n\")\n",
    "        \n",
    "tfidf_run = []\n",
    "for _, row in topics_df.iterrows():\n",
    "    qid, query = row\n",
    "    res_df = tfidf.search(query)\n",
    "    for _, res_row in res_df.iterrows():\n",
    "        _, docid, docno, rank, score, query = res_row\n",
    "        row_str = f\"{qid} 0 {docno} {rank} {score} tfidf\"\n",
    "        tfidf_run.append(row_str)\n",
    "with open(\"outputs/tfidf.run\", \"w\") as f:\n",
    "    for l in tfidf_run:\n",
    "        f.write(l + \"\\n\")\n",
    "\n",
    "bm25_run = []\n",
    "for _, row in topics_df.iterrows():\n",
    "    qid, query = row\n",
    "    res_df = bm25.search(query)\n",
    "    for _, res_row in res_df.iterrows():\n",
    "        _, docid, docno, rank, score, query = res_row\n",
    "        row_str = f\"{qid} 0 {docno} {rank} {score} tfidf\"\n",
    "        bm25_run.append(row_str)\n",
    "with open(\"outputs/bm25.run\", \"w\") as f:\n",
    "    for l in bm25_run:\n",
    "        f.write(l + \"\\n\")\n",
    "\n",
    "bm25_run[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytrec_eval\n",
    "\n",
    "[pytrec_eval](https://github.com/cvangysel/pytrec_eval) is a Python interface to TREC's evaluation tool [`trec_eval`](https://github.com/usnistgov/trec_eval).\n",
    "You can install it as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytrec_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytrec_eval requires three arguments:\n",
    "- qrel: a dictionary mapping each query id to the relevant documents and their labels. For example:\n",
    "```python\n",
    "qrel = {\n",
    "    'q1': {'d1': 0, 'd2': 1, 'd3': 0},\n",
    "    'q2': {'d2': 1, 'd3': 1},\n",
    "}\n",
    "```\n",
    "- metrics: a set of standard metrics to be used to assess your system. See [here](http://www.rafaelglater.com/en/post/learn-how-to-use-trec_eval-to-evaluate-your-information-retrieval-system) for a list of available metrics.\n",
    "- run: similar to `qrel`, this is a dictionary of a given run which maps each query id to the relevant documents and their scores. For example:\n",
    "```python\n",
    "run = {\n",
    "    'q1': {'d1': 1.0, 'd2': 0.0, 'd3': 1.5},\n",
    "    'q2': {'d1': 1.5, 'd2': 0.2, 'd3': 0.5}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load qrels\n",
    "qrels_df = pd.read_csv('data/lab_qrels.csv', dtype=str)\n",
    "print(qrels_df.shape)\n",
    "print(qrels_df.head())\n",
    "\n",
    "qrels_dict = dict()\n",
    "for _, r in qrels_df.iterrows():\n",
    "    qid, docno, label, iteration = r\n",
    "    if qid not in qrels_dict:\n",
    "        qrels_dict[qid] = dict()\n",
    "    qrels_dict[qid][docno] = int(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out `pytrec_eval.parse_qrel()` to quickly load qrels files in TREC format (as in your project)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytrec_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build evaluator based on the qrels and metrics\n",
    "metrics = {\"map\", \"ndcg\", \"ndcg_cut_10\", \"P_10\"}\n",
    "my_qrel = {q: d for q, d in qrels_dict.items() if q in {'1015979', '2674', '340095'}}  # let's evaluate the first 3 topics to compare with PyTerrier above\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(my_qrel, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load run\n",
    "with open(\"outputs/tf.run\", 'r') as f_run:\n",
    "    tf_run = pytrec_eval.parse_run(f_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tf model\n",
    "tf_evals = evaluator.evaluate(tf_run)\n",
    "tf_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_metric2vals = {m: [] for m in metrics}\n",
    "for q, d in tf_evals.items():\n",
    "    for m, val in d.items():\n",
    "        tf_metric2vals[m].append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average across topics\n",
    "for m in metrics:\n",
    "    print(m, '\\t', pytrec_eval.compute_aggregated_measure(m, tf_metric2vals[m]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
